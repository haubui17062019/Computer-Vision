{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx\n",
        "help(torch.onnx.export)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ys5TCAVPUcx",
        "outputId": "cbb7ff0a-2ce9-4bf4-ca39-8e22cee3c869"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function export in module torch.onnx.utils:\n",
            "\n",
            "export(model: 'Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction]', args: 'Union[Tuple[Any, ...], torch.Tensor]', f: 'Union[str, io.BytesIO]', export_params: 'bool' = True, verbose: 'bool' = False, training: '_C_onnx.TrainingMode' = <TrainingMode.EVAL: 0>, input_names: 'Optional[Sequence[str]]' = None, output_names: 'Optional[Sequence[str]]' = None, operator_export_type: '_C_onnx.OperatorExportTypes' = <OperatorExportTypes.ONNX: 0>, opset_version: 'Optional[int]' = None, do_constant_folding: 'bool' = True, dynamic_axes: 'Optional[Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]]' = None, keep_initializers_as_inputs: 'Optional[bool]' = None, custom_opsets: 'Optional[Mapping[str, int]]' = None, export_modules_as_functions: 'Union[bool, Collection[Type[torch.nn.Module]]]' = False) -> 'None'\n",
            "    Exports a model into ONNX format.\n",
            "    \n",
            "    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\n",
            "    :class:`torch.jit.ScriptFunction`, this runs\n",
            "    ``model`` once in order to convert it to a TorchScript graph to be exported\n",
            "    (the equivalent of :func:`torch.jit.trace`). Thus this has the same limited support\n",
            "    for dynamic control flow as :func:`torch.jit.trace`.\n",
            "    \n",
            "    Args:\n",
            "        model (:class:`torch.nn.Module`, :class:`torch.jit.ScriptModule` or :class:`torch.jit.ScriptFunction`):\n",
            "            the model to be exported.\n",
            "        args (tuple or torch.Tensor):\n",
            "    \n",
            "            args can be structured either as:\n",
            "    \n",
            "            1. ONLY A TUPLE OF ARGUMENTS::\n",
            "    \n",
            "                args = (x, y, z)\n",
            "    \n",
            "            The tuple should contain model inputs such that ``model(*args)`` is a valid\n",
            "            invocation of the model. Any non-Tensor arguments will be hard-coded into the\n",
            "            exported model; any Tensor arguments will become inputs of the exported model,\n",
            "            in the order they occur in the tuple.\n",
            "    \n",
            "            2. A TENSOR::\n",
            "    \n",
            "                args = torch.Tensor([1])\n",
            "    \n",
            "            This is equivalent to a 1-ary tuple of that Tensor.\n",
            "    \n",
            "            3. A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS::\n",
            "    \n",
            "                args = (\n",
            "                    x,\n",
            "                    {\n",
            "                        \"y\": input_y,\n",
            "                        \"z\": input_z\n",
            "                    }\n",
            "                )\n",
            "    \n",
            "            All but the last element of the tuple will be passed as non-keyword arguments,\n",
            "            and named arguments will be set from the last element. If a named argument is\n",
            "            not present in the dictionary, it is assigned the default value, or None if a\n",
            "            default value is not provided.\n",
            "    \n",
            "            .. note::\n",
            "                If a dictionary is the last element of the args tuple, it will be\n",
            "                interpreted as containing named arguments. In order to pass a dict as the\n",
            "                last non-keyword arg, provide an empty dict as the last element of the args\n",
            "                tuple. For example, instead of::\n",
            "    \n",
            "                    torch.onnx.export(\n",
            "                        model,\n",
            "                        (\n",
            "                            x,\n",
            "                            # WRONG: will be interpreted as named arguments\n",
            "                            {y: z}\n",
            "                        ),\n",
            "                        \"test.onnx.pb\"\n",
            "                    )\n",
            "    \n",
            "                Write::\n",
            "    \n",
            "                    torch.onnx.export(\n",
            "                        model,\n",
            "                        (\n",
            "                            x,\n",
            "                            {y: z},\n",
            "                            {}\n",
            "                        ),\n",
            "                        \"test.onnx.pb\"\n",
            "                    )\n",
            "    \n",
            "        f: a file-like object (such that ``f.fileno()`` returns a file descriptor)\n",
            "            or a string containing a file name.  A binary protocol buffer will be written\n",
            "            to this file.\n",
            "        export_params (bool, default True): if True, all parameters will\n",
            "            be exported. Set this to False if you want to export an untrained model.\n",
            "            In this case, the exported model will first take all of its parameters\n",
            "            as arguments, with the ordering as specified by ``model.state_dict().values()``\n",
            "        verbose (bool, default False): if True, prints a description of the\n",
            "            model being exported to stdout. In addition, the final ONNX graph will include the\n",
            "            field ``doc_string``` from the exported model which mentions the source code locations\n",
            "            for ``model``. If True, ONNX exporter logging will be turned on.\n",
            "        training (enum, default TrainingMode.EVAL):\n",
            "            * ``TrainingMode.EVAL``: export the model in inference mode.\n",
            "            * ``TrainingMode.PRESERVE``: export the model in inference mode if model.training is\n",
            "                False and in training mode if model.training is True.\n",
            "            * ``TrainingMode.TRAINING``: export the model in training mode. Disables optimizations\n",
            "                which might interfere with training.\n",
            "        input_names (list of str, default empty list): names to assign to the\n",
            "            input nodes of the graph, in order.\n",
            "        output_names (list of str, default empty list): names to assign to the\n",
            "            output nodes of the graph, in order.\n",
            "        operator_export_type (enum, default OperatorExportTypes.ONNX):\n",
            "    \n",
            "            * ``OperatorExportTypes.ONNX``: Export all ops as regular ONNX ops\n",
            "                (in the default opset domain).\n",
            "            * ``OperatorExportTypes.ONNX_FALLTHROUGH``: Try to convert all ops\n",
            "                to standard ONNX ops in the default opset domain. If unable to do so\n",
            "                (e.g. because support has not been added to convert a particular torch op to ONNX),\n",
            "                fall back to exporting the op into a custom opset domain without conversion. Applies\n",
            "                to `custom ops <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_\n",
            "                as well as ATen ops. For the exported model to be usable, the runtime must support\n",
            "                these non-standard ops.\n",
            "            * ``OperatorExportTypes.ONNX_ATEN``: All ATen ops (in the TorchScript namespace \"aten\")\n",
            "                are exported as ATen ops (in opset domain \"org.pytorch.aten\").\n",
            "                `ATen <https://pytorch.org/cppdocs/#aten>`_ is PyTorch's built-in tensor library, so\n",
            "                this instructs the runtime to use PyTorch's implementation of these ops.\n",
            "    \n",
            "                .. warning::\n",
            "    \n",
            "                    Models exported this way are probably runnable only by Caffe2.\n",
            "    \n",
            "                    This may be useful if the numeric differences in implementations of operators are\n",
            "                    causing large differences in behavior between PyTorch and Caffe2 (which is more\n",
            "                    common on untrained models).\n",
            "    \n",
            "            * ``OperatorExportTypes.ONNX_ATEN_FALLBACK``: Try to export each ATen op\n",
            "                (in the TorchScript namespace \"aten\") as a regular ONNX op. If we are unable to do so\n",
            "                (e.g. because support has not been added to convert a particular torch op to ONNX),\n",
            "                fall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for\n",
            "                context.\n",
            "                For example::\n",
            "    \n",
            "                    graph(%0 : Float):\n",
            "                    %3 : int = prim::Constant[value=0]()\n",
            "                    # conversion unsupported\n",
            "                    %4 : Float = aten::triu(%0, %3)\n",
            "                    # conversion supported\n",
            "                    %5 : Float = aten::mul(%4, %0)\n",
            "                    return (%5)\n",
            "    \n",
            "                Assuming ``aten::triu`` is not supported in ONNX, this will be exported as::\n",
            "    \n",
            "                    graph(%0 : Float):\n",
            "                    %1 : Long() = onnx::Constant[value={0}]()\n",
            "                    # not converted\n",
            "                    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1)\n",
            "                    # converted\n",
            "                    %3 : Float = onnx::Mul(%2, %0)\n",
            "                    return (%3)\n",
            "    \n",
            "                If PyTorch was built with Caffe2 (i.e. with ``BUILD_CAFFE2=1``), then\n",
            "                Caffe2-specific behavior will be enabled, including special support\n",
            "                for ops are produced by the modules described in\n",
            "                `Quantization <https://pytorch.org/docs/stable/quantization.html>`_.\n",
            "    \n",
            "                .. warning::\n",
            "    \n",
            "                    Models exported this way are probably runnable only by Caffe2.\n",
            "    \n",
            "        opset_version (int, default 14): The version of the\n",
            "            `default (ai.onnx) opset <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_\n",
            "            to target. Must be >= 7 and <= 16.\n",
            "        do_constant_folding (bool, default True): Apply the constant-folding optimization.\n",
            "            Constant-folding will replace some of the ops that have all constant inputs\n",
            "            with pre-computed constant nodes.\n",
            "        dynamic_axes (dict[string, dict[int, string]] or dict[string, list(int)], default empty dict):\n",
            "    \n",
            "            By default the exported model will have the shapes of all input and output tensors\n",
            "            set to exactly match those given in ``args``. To specify axes of tensors as\n",
            "            dynamic (i.e. known only at run-time), set ``dynamic_axes`` to a dict with schema:\n",
            "    \n",
            "            * KEY (str): an input or output name. Each name must also be provided in ``input_names`` or\n",
            "                ``output_names``.\n",
            "            * VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a\n",
            "                list, each element is an axis index.\n",
            "    \n",
            "            For example::\n",
            "    \n",
            "                class SumModule(torch.nn.Module):\n",
            "                    def forward(self, x):\n",
            "                        return torch.sum(x, dim=1)\n",
            "    \n",
            "                torch.onnx.export(\n",
            "                    SumModule(),\n",
            "                    (torch.ones(2, 2),),\n",
            "                    \"onnx.pb\",\n",
            "                    input_names=[\"x\"],\n",
            "                    output_names=[\"sum\"]\n",
            "                )\n",
            "    \n",
            "            Produces::\n",
            "    \n",
            "                input {\n",
            "                  name: \"x\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 0\n",
            "                        }\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 1\n",
            "                ...\n",
            "                output {\n",
            "                  name: \"sum\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 0\n",
            "                ...\n",
            "    \n",
            "            While::\n",
            "    \n",
            "                torch.onnx.export(\n",
            "                    SumModule(),\n",
            "                    (torch.ones(2, 2),),\n",
            "                    \"onnx.pb\",\n",
            "                    input_names=[\"x\"],\n",
            "                    output_names=[\"sum\"],\n",
            "                    dynamic_axes={\n",
            "                        # dict value: manually named axes\n",
            "                        \"x\": {0: \"my_custom_axis_name\"},\n",
            "                        # list value: automatic names\n",
            "                        \"sum\": [0],\n",
            "                    }\n",
            "                )\n",
            "    \n",
            "            Produces::\n",
            "    \n",
            "                input {\n",
            "                  name: \"x\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_param: \"my_custom_axis_name\"  # axis 0\n",
            "                        }\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 1\n",
            "                ...\n",
            "                output {\n",
            "                  name: \"sum\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n",
            "                ...\n",
            "    \n",
            "        keep_initializers_as_inputs (bool, default None): If True, all the\n",
            "            initializers (typically corresponding to parameters) in the\n",
            "            exported graph will also be added as inputs to the graph. If False,\n",
            "            then initializers are not added as inputs to the graph, and only\n",
            "            the non-parameter inputs are added as inputs.\n",
            "            This may allow for better optimizations (e.g. constant folding) by\n",
            "            backends/runtimes.\n",
            "    \n",
            "            If ``opset_version < 9``, initializers MUST be part of graph\n",
            "            inputs and this argument will be ignored and the behavior will be\n",
            "            equivalent to setting this argument to True.\n",
            "    \n",
            "            If None, then the behavior is chosen automatically as follows:\n",
            "    \n",
            "            * If ``operator_export_type=OperatorExportTypes.ONNX``, the behavior is equivalent\n",
            "                to setting this argument to False.\n",
            "            * Else, the behavior is equivalent to setting this argument to True.\n",
            "    \n",
            "        custom_opsets (dict[str, int], default empty dict): A dict with schema:\n",
            "    \n",
            "            * KEY (str): opset domain name\n",
            "            * VALUE (int): opset version\n",
            "    \n",
            "            If a custom opset is referenced by ``model`` but not mentioned in this dictionary,\n",
            "            the opset version is set to 1. Only custom opset domain name and version should be\n",
            "            indicated through this argument.\n",
            "    \n",
            "        export_modules_as_functions (bool or set of type of nn.Module, default False): Flag to enable\n",
            "            exporting all ``nn.Module`` forward calls as local functions in ONNX. Or a set to indicate the\n",
            "            particular types of modules to export as local functions in ONNX.\n",
            "            This feature requires ``opset_version`` >= 15, otherwise the export will fail. This is because\n",
            "            ``opset_version`` < 15 implies IR version < 8, which means no local function support.\n",
            "            Module variables will be exported as function attributes. There are two categories of function\n",
            "            attributes.\n",
            "    \n",
            "            1. Annotated attributes: class variables that have type annotations via\n",
            "            `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_\n",
            "            will be exported as attributes.\n",
            "            Annotated attributes are not used inside the subgraph of ONNX local function because\n",
            "            they are not created by PyTorch JIT tracing, but they may be used by consumers\n",
            "            to determine whether or not to replace the function with a particular fused kernel.\n",
            "    \n",
            "            2. Inferred attributes: variables that are used by operators inside the module. Attribute names\n",
            "            will have prefix \"inferred::\". This is to differentiate from predefined attributes retrieved from\n",
            "            python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\n",
            "    \n",
            "            * ``False`` (default): export ``nn.Module`` forward calls as fine grained nodes.\n",
            "            * ``True``: export all ``nn.Module`` forward calls as local function nodes.\n",
            "            * Set of type of nn.Module: export ``nn.Module`` forward calls as local function nodes,\n",
            "                only if the type of the ``nn.Module`` is found in the set.\n",
            "    \n",
            "    Raises:\n",
            "        :class:`torch.onnx.errors.CheckerError`: If the ONNX checker detects an invalid ONNX graph.\n",
            "        :class:`torch.onnx.errors.UnsupportedOperatorError`: If the ONNX graph cannot be exported because it\n",
            "            uses an operator that is not supported by the exporter.\n",
            "        :class:`torch.onnx.errors.OnnxExporterError`: Other errors that can occur during export.\n",
            "            All errors are subclasses of :class:`errors.OnnxExporterError`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dùng thử trên AlexNet"
      ],
      "metadata": {
        "id": "MeWmAHKJPuMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xnt7wWzQkp_",
        "outputId": "2505269f-d6c1-485b-9e48-69f7de2a55ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "# image have shape (224, 224, 3)\n",
        "# set up inputs\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# call model\n",
        "model = torchvision.models.alexnet(pretrained=True)\n",
        "# convert to onnx\n",
        "torch.onnx.export(model, dummy_input, 'alexnet.onnx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bho-lxUPo_I",
        "outputId": "8ab3b0ff-3f76-4ac2-f533-6966ceee6f00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### kiểm tra mô hình"
      ],
      "metadata": {
        "id": "EBemkAgVQxoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# load model ONNX \n",
        "model = onnx.load('/content/alexnet.onnx')\n",
        "\n",
        "# kiểm tra \n",
        "onnx.checker.check_model(model)\n",
        "\n",
        "# in biểu đồ thị\n",
        "print(onnx.helper.printable_graph(model.graph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vibjoaZxQKIR",
        "outputId": "ebbb6c4c-de80-4a0c-c645-b1e1b7ea3610"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph torch_jit (\n",
            "  %input.1[FLOAT, 1x3x224x224]\n",
            ") initializers (\n",
            "  %features.0.weight[FLOAT, 64x3x11x11]\n",
            "  %features.0.bias[FLOAT, 64]\n",
            "  %features.3.weight[FLOAT, 192x64x5x5]\n",
            "  %features.3.bias[FLOAT, 192]\n",
            "  %features.6.weight[FLOAT, 384x192x3x3]\n",
            "  %features.6.bias[FLOAT, 384]\n",
            "  %features.8.weight[FLOAT, 256x384x3x3]\n",
            "  %features.8.bias[FLOAT, 256]\n",
            "  %features.10.weight[FLOAT, 256x256x3x3]\n",
            "  %features.10.bias[FLOAT, 256]\n",
            "  %classifier.1.weight[FLOAT, 4096x9216]\n",
            "  %classifier.1.bias[FLOAT, 4096]\n",
            "  %classifier.4.weight[FLOAT, 4096x4096]\n",
            "  %classifier.4.bias[FLOAT, 4096]\n",
            "  %classifier.6.weight[FLOAT, 1000x4096]\n",
            "  %classifier.6.bias[FLOAT, 1000]\n",
            ") {\n",
            "  %/features/features.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [11, 11], pads = [2, 2, 2, 2], strides = [4, 4]](%input.1, %features.0.weight, %features.0.bias)\n",
            "  %/features/features.1/Relu_output_0 = Relu(%/features/features.0/Conv_output_0)\n",
            "  %/features/features.2/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.1/Relu_output_0)\n",
            "  %/features/features.3/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/features/features.2/MaxPool_output_0, %features.3.weight, %features.3.bias)\n",
            "  %/features/features.4/Relu_output_0 = Relu(%/features/features.3/Conv_output_0)\n",
            "  %/features/features.5/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.4/Relu_output_0)\n",
            "  %/features/features.6/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.5/MaxPool_output_0, %features.6.weight, %features.6.bias)\n",
            "  %/features/features.7/Relu_output_0 = Relu(%/features/features.6/Conv_output_0)\n",
            "  %/features/features.8/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.7/Relu_output_0, %features.8.weight, %features.8.bias)\n",
            "  %/features/features.9/Relu_output_0 = Relu(%/features/features.8/Conv_output_0)\n",
            "  %/features/features.10/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.9/Relu_output_0, %features.10.weight, %features.10.bias)\n",
            "  %/features/features.11/Relu_output_0 = Relu(%/features/features.10/Conv_output_0)\n",
            "  %/features/features.12/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.11/Relu_output_0)\n",
            "  %/avgpool/AveragePool_output_0 = AveragePool[kernel_shape = [1, 1], strides = [1, 1]](%/features/features.12/MaxPool_output_0)\n",
            "  %/Flatten_output_0 = Flatten[axis = 1](%/avgpool/AveragePool_output_0)\n",
            "  %/classifier/classifier.1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/Flatten_output_0, %classifier.1.weight, %classifier.1.bias)\n",
            "  %/classifier/classifier.2/Relu_output_0 = Relu(%/classifier/classifier.1/Gemm_output_0)\n",
            "  %/classifier/classifier.4/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.2/Relu_output_0, %classifier.4.weight, %classifier.4.bias)\n",
            "  %/classifier/classifier.5/Relu_output_0 = Relu(%/classifier/classifier.4/Gemm_output_0)\n",
            "  %36 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.5/Relu_output_0, %classifier.6.weight, %classifier.6.bias)\n",
            "  return %36\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hbFy926RFGi",
        "outputId": "5025ff30-81c9-446d-9db0-4aef3d83a89d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting netron\n",
            "  Downloading netron-6.8.8-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: netron\n",
            "Successfully installed netron-6.8.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xem trực quan kiến trúc\n"
      ],
      "metadata": {
        "id": "DC6fJFZqVWhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import netron\n",
        "netron.start('/content/alexnet.onnx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ih9V-zBT0j-",
        "outputId": "d35befcb-fb0e-4677-fe1e-7e5584d7fd9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving '/content/alexnet.onnx' at http://localhost:8081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('localhost', 8081)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}